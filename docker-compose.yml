version: "3.7"
services:
    postgres:
        container_name: postgres
        image: postgres:9.6
        environment:
            - POSTGRES_USER=${AIRFLOW}
            - POSTGRES_PASSWORD=${AIRFLOW}
            - POSTGRES_DB=${AIRFLOW}
        logging:
            options:
                max-size: 10m
                max-file: "3"        

    airflow:
        container_name: airflow
        build: ./airflow
        restart: always
        depends_on:
            - postgres
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
            - ./airflow/dags:/usr/local/airflow/dags
            - ./airflow/modules:/usr/local/airflow/modules
        ports:
            - "8080:8080"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    zookeeper:
        container_name: zookeeper
        image: confluentinc/cp-zookeeper:5.3.1
        environment:
            - ZOOKEEPER_CLIENT_PORT=${ZOOKEEPER_CLIENT_PORT}
        ports:
            - "2181:2181"
        volumes:
            - zookeeper_data:/var/lib/zookeeper/data

    kafka:
        container_name: kafka
        build: ./kafka
        environment: 
            - KAFKA_CREATE_TOPICS=${KAFKA_CREATE_TOPICS}
            - KAFKA_ZOOKEEPER_CONNECT=${ZOOKEEPER_URL}
            - KAFKA_ADVERTISED_LISTENERS=${KAFKA_ADVERTISED_LISTENERS}
        depends_on:
            - zookeeper
        volumes:
            - kafka_data:/kafka/data  

    schema-registry:
        container_name: schema-registry
        image: confluentinc/cp-schema-registry:5.3.1
        environment:
            - SCHEMA_REGISTRY_HOST_NAME=${SCHEMA_REGISTRY_HOST_NAME}
            - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=${ZOOKEEPER_URL}
            - SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL=${SCHEMA_REGISTRY_LOGLEVEL}
        depends_on:
            - kafka  

    redis:
        container_name: redis
        image: redis:alpine
        command: redis-server
        ports:
            - "6379:6379"
        volumes:
            - ./redis:/usr/local/etc/redis

    connect:
        container_name: connect
        build: ./connect
        environment:
            - CONNECT_REST_ADVERTISED_HOST_NAME=${CONNECT_REST_ADVERTISED_HOST_NAME}
            - CONNECT_SCHEMA_REGISTRY_URL=${SCHEMA_REGISTRY_URL}
            - CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL=${SCHEMA_REGISTRY_URL}
            - CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL=${SCHEMA_REGISTRY_URL}
            - CONNECT_BOOTSTRAP_SERVERS=${CONNECT_BOOTSTRAP_SERVERS} 
        ports:
            - "8083:8083"
        depends_on:
            - kafka
            - schema-registry

    elasticsearch:
        container_name: elasticsearch
        build: ./elastic
        restart: unless-stopped
        ulimits:
            memlock:
                soft: -1
                hard: -1
        ports:
            - 9200:9200

    mongo:
        container_name: mongo
        image: mongo:4.2
        ports:
            - "27017:27017"
        command: ["mongod", "--replSet", "rs0", "--auth"]
        volumes:
            - ./mongo/init.sh:/usr/local/bin/init.sh
    
    api:
        container_name: api
        build: ./api
        command: >
            sh -c "python manage.py makemigrations &&
            python manage.py migrate &&
            python manage.py runserver 0.0.0.0:5000"
        ports:
            - "5000:5000"
        environment:
            - ELASTIC_HOST=${ELASTIC_HOST}
            - ELASTIC_INDEX=${ELASTIC_INDEX}
            - MONGO_HOST=${MONGO_HOST}
            - MONGO_USR=${RSS_NEWS}
            - MONGO_PASSWD=${RSS_NEWS}
        depends_on:
            - mongo
            - elasticsearch
        volumes:
            - ./api/app:/app

networks: 
    default:
        name: backend-net

volumes:
    zookeeper_data: {}
    kafka_data: {}
    redis: {}
